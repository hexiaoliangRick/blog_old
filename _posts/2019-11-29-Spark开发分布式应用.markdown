---  
layout:     post  
title:      "Spark学习记录"  
subtitle:   "Spark学习记录"  
date:       2019-11-29 12:00:00 
author:     "cpuRick"  
header-img: "img/post-bg-2015.jpg"  
tags:  
    - Spark,大数据  
---    
# Spark学习记录
--------
##Docker搭建Hadoop3.0完全分布式集群
####参考文档
1.博客文章：https://www.cnblogs.com/onetwo/p/6419925.html    
2.官方文档：https://hadoop.apache.org/docs/r0.23.11/hadoop-yarn/hadoop-yarn-common/yarn-default.xml
3.对外服务端口：http://localhost:9870/dfshealth.html#tab-overview
####实际操作
##### 单机集群搭建以及运行wordcount例子
1. 参考指导：https://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-common/SingleCluster.html
   如果网页在google中显示有问题，可以选择用ie浏览器试试。   
2. step by step 的进行配置。
3. 操作过程中遇到的问题总结
 - 提交任务之后，连接不上Resource Manager ，报错
 ```cluster-master:18040 failed on connection exception: java.net.ConnectException: Connection refused; For                                                                                                                         more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBCli                                                                                                                        entImpl.getNewApplication over null after 1 failover attempts. ```

 ![hadoop3.0-rs连接不上报错](../img/gis/spark/hadoop3.0-rs连接不上报错.png)
 ---
 **原因**  
 在启动集群的时候只调用了start-dfs.sh脚本，没有调用start-all.sh脚本，导致集群只启动了hdfs的服务，没有启动rs相关的服务。
 - Hadoop集群安全模式下，提交任务失败 ```org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/root/grep-temp-1689422376. Name node is in safe mode.```
 ![hadoop-3.0集群安全模式](../img/gis/spark/hadoop-3.0集群安全模式.png)
 **原因**  
 安全模式是集群自我保护的一种机制，在安全模式下，不允许对文件进行删除操作。在NameNode主节点启动时，HDFS首先进入安全模式，DataNode在启动的时候会向namenode汇报可用的block等状态，当整个系统达到安全标准时，HDFS自动离开安全模式。
 在我的操作中，集群刚启动，我就开始提交任务，这时候，集群必须删除上一次任务执行的记录信息，但是处在安全模式下，所以任务运行失败。
 **如何退出安全模式，如何查看当前是否处于安全模式**
 https://blog.csdn.net/bingduanlbd/article/details/51900512
 - mapred-site.xml文件中没有配置，map、reduce运行的环境变量，导致任务运行失败。`NFO mapreduce.Job: Job job_1575113633802_0002 failed with state FAILED due to: Application application_1575113633802_0002 failed 2 times due to AM Container for appattempt_1575113633802_0002_000002 exited with  exitCode: 1
                                                      Failing this attempt.Diagnostics: [2019-11-30 11:40:06.901]Exception from container-launch.
                                                      Container id: container_1575113633802_0002_02_000001
                                                      Exit code: 1
                                                      [2019-11-30 11:40:06.907]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
                                                      Last 4096 bytes of prelaunch.err :
                                                      Last 4096 bytes of stderr :
                                                      Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster`
 ![hadoop-3.0Mapreduce环境变量](../img/gis/spark/hadoop-3.0Mapreduce环境变量.png)
 **原因**   
 https://mathsigit.github.io/blog_page/2017/11/16/hole-of-submitting-mr-of-hadoop300RC0/
 **In Hadoop 3, YARN containers do not inherit the NodeManagers’ environment variables. Therefore, if you want to inherit NodeManager’s environment variables (e.g. HADOOP_MAPRED_HOME), you need to set additional parameters (e.g. mapreduce.admin.user.env and yarn.app.mapreduce.am.env).**
 3.0集群，不支持环境变量的继承，所以我们需要在配置文件中添加配置项。
 - 调用start-all.sh脚本之后，集群只启动namenode、secondnamenode服务，没有datanode服务运行。
 **原因**
 数据不一致，删除集群环境下，dfs数据存储的位置。
 集群的启动、关闭要通过脚本执行，不能强制杀死进程。
 ##### 分布式集群搭建
  配置和单机基本相同，主要修改以下几点
  - 配置/etc/hosts文件，使集群之间能够互相ping通   
      `127.0.0.1   localhost  
      
       172.18.0.2  cluster-master
         
       172.18.0.3  cluster-slave1
       
       172.18.0.4  cluster-slave2
       
       172.18.0.5  cluster-slave3`
   
  - /etc/workers 文件配置集群
  - /opt/hadoop/etc/hadoop/hdfs-site.xml中集群数量的配置   
  ![集群-数量](../img/gis/spark/集群-数量.png)
  - 配置各主机间能够免密登陆
  - 修改~/.bashrc 文件，增加HADOOP_HOME 和JAVA_HOME的环境变量
          ```
        # User specific aliases and functions
        
        alias rm='rm -i'
        alias cp='cp -i'
        alias mv='mv -i'
        
        # Source global definitions
        if [ -f /etc/bashrc ]; then
                . /etc/bashrc
        fi
        :>/etc/hosts
        cat >>/etc/hosts<<EOF
        127.0.0.1   localhost
        172.18.0.2  cluster-master
        172.18.0.3  cluster-slave1
        172.18.0.4  cluster-slave2
        172.18.0.5  cluster-slave3
        EOF
        # hadoop
        export HADOOP_HOME=/opt/hadoop-3.1.2
        export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
        
        #java
        export JAVA_HOME=/opt/jdk1.8.0_221
        export PATH=$JAVA_HOME/bin:$PATH
        ```
  
 
####知识总结
1. Hadoop端口只能本地链接问题：https://www.cnblogs.com/duanxz/p/5142535.html
2. docker 镜像导入、导出问题：https://blog.csdn.net/wang725/article/details/83098685
<!--
 * @Author: your name
 * @Date: 2020-02-19 11:08:29
 * @LastEditTime: 2020-02-23 19:30:33
 * @LastEditors: Please set LastEditors
 * @Description: In User Settings 
 * @FilePath: \machinelearning\机器学习.markdown
 -->
# 机器学习

## 机器学习类型
- 监督学习    
  训练集包含特征和目标，且目标人为标注，根据训练集学习一个函数，当心的数据到来的时候，根据函数预测结果。
  
- 无监督学习   
  有训练集，有输入和输出；与监督学习相比，训练集没有人为标注。
- 半监督学习  
  介于监督学习和无监督学习之间。  
- 强化学习  
  通过观察来学习做成某种动作，每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。
## K近邻算法

## 决策树
- 决策树构建算法  
 1. ID3
 2. C4.5
 3. CART
- 特征选择
 选取哪个特征来划分特征空间，目的在于选取对于数据有分类能力的特征。

- 决策树构建
 1. 计算每个标签的信息熵
 2. 计算每个特征（属性）的信息增益
 3. 根据信息增益来选择划分的特征
 4. 递归构建决策树


- 决策树剪枝
 通过训练数据集构建决策树，过多的考虑了训练数据的正确性。所以通过决策树对测试数据进行分类的时候，会产生过拟合现象，导致分类不准确，因此，我们需要进行剪枝处理。减枝又分为自上而下的减枝和自下而上的剪枝。
## 朴素贝叶斯算法
- 原理   
  根据朴素贝叶斯公式计算后验概率
    $$ P_{(A|B)}=P_{(A)}* \frac{P_{(B|A)}}{P_{(B)}} $$
  1. $P_{(A|B)}$称为**后验概率**
  2. $P_{(A)}$称为**先验概率**
  3. $\frac{P_{(B|A)}}{P_{(B)}}$ 称为**调整因子**
  
  其中$P_{(B)}$由全概率公式求得:
   $$P_{(B)}=\sum _{i=1} ^n P_{B|A_{i}} $$
- 朴素贝叶斯算法的种类  
  在**sklearn**中有三种算法：高斯(GaussianNB)、多项式(multinominalNB)、伯努利(BernoulliNB)
  1. GaussianNB   
      高斯贝叶斯的含义是，假设每个标签的数据都服从高斯分布，也就是正态分布。即先验为高斯分布函数
      $$
      P_{X_j=x_j|Y=C_k}=\frac{1}{\sqrt{2*\pi*{\sigma}_k^2}}
      \exp(-\frac{(x_j-u_k)^2}{2*\sigma_k^2})
      $$ 
      
      其中$C_K$为Y的第k类类别，$u_k$和$\sigma_k^2$为需要从从训练集中计算的平均值和方差。高维正态分布函数下是**梯度** 和 **协方差矩阵Hissen**    

      **后验概率也能是特征在标签下的条件概率**
      

  2. MultinominalNB   
    假设：特征是由一个简单的多项式分布生成的，多项分布可以描述各类型样本出现**次数**的概率。非常适合描述出现次数或者出现次数比例的特征。常用于文本分类，词出现的次数作为特征。  
    $x_{jl}$为特征出现的次数$m_k$是训练集中输出为第k类的样本个数。 $\lambda$的作用是为了防止分子分母为零的情况下出现，当$\lambda$为1的时候，就为拉普拉斯平滑。
    $$
        P_{X_j=x_j|Y=C_k}=\frac{x_{jl}+\lambda}{m_k+\lambda}
    $$
    

  3. BernoulliNB  
  就是先验为伯努利分布的朴素贝叶斯，假设特征的先验为二元的伯努利分布，
  在伯努利模型中，每个特征只有两种取值，true/false 0或者1，在文本分类中表示某个词在文档中有没有出现。
  $$
        P_{X_j=x_jl|Y=C_k}=P_{(j|Y=C_k)}x_{jl}+(1-P_{(j|Y=C_k)})(1-x_{jl})
    $$
  4. 总结     
     - 样本分布是连续值用高斯比较好
     - 样本分布是多元离散值使用多项式比较好
     - 样本特征是二元离散值或者很稀疏的多元离散值，用伯努利比较好
  
  
     



